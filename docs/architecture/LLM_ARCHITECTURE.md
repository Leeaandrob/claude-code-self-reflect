# Arquitetura de LLM: Claude Code + Claude Self-Reflect

## TL;DR - Resposta R√°pida

**Pergunta**: Como est√° a camada de LLM? √â bind ao Claude Code ou chamamos a API da Anthropic?

**Resposta**:
1. **Claude Code** ‚Üí Chama API da Anthropic (ou Bedrock/Vertex)
2. **Claude Self-Reflect MCP Server** ‚Üí **N√ÉO** chama LLM algum (exceto batch narratives v7.0)
3. **Separa√ß√£o clara**: Claude Code = Intelig√™ncia | MCP Server = Mem√≥ria/Busca

---

## 1. Arquitetura Completa

### 1.1 Vis√£o Geral

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Usu√°rio                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Code (CLI)                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  MCP Client (stdio/http)                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Gerencia conex√µes com MCP servers                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Passa resultados para o LLM                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Renderiza respostas para o usu√°rio                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  LLM Backend (AQUI EST√Å O CLAUDE!)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Op√ß√£o 1: Anthropic API (Padr√£o)                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ api.anthropic.com                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Modelos: Sonnet 4.5, Haiku 4.5                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Auth: ANTHROPIC_API_KEY                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Op√ß√£o 2: AWS Bedrock                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ bedrock-runtime.us-east-1.amazonaws.com             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Modelos: claude-sonnet-4-5, claude-haiku-4-5       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Auth: AWS credentials                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Op√ß√£o 3: Google Vertex AI                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ vertex-ai.googleapis.com                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Auth: Google Cloud credentials                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ stdio/http
        ‚îÇ Invoca MCP tools quando necess√°rio
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Claude Self-Reflect MCP Server (Python)               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚ùå N√ÉO TEM LLM PR√ìPRIO                                        ‚îÇ
‚îÇ  ‚úÖ Apenas processamento de dados:                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  1. Embeddings:                                                ‚îÇ
‚îÇ     ‚îú‚îÄ Local: FastEmbed (all-MiniLM-L6-v2) 384d               ‚îÇ
‚îÇ     ‚îî‚îÄ Cloud: Voyage AI (voyage-3) 1024d                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. Busca Vetorial: Qdrant                                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. Processamento: Python puro                                 ‚îÇ
‚îÇ     ‚îú‚îÄ Parsing de JSONL                                        ‚îÇ
‚îÇ     ‚îú‚îÄ Chunking de conversas                                   ‚îÇ
‚îÇ     ‚îú‚îÄ Extra√ß√£o de metadata                                    ‚îÇ
‚îÇ     ‚îî‚îÄ Formatar resultados                                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  üÜï EXCE√á√ÉO: v7.0 Batch Narratives                            ‚îÇ
‚îÇ     ‚îî‚îÄ Anthropic Batch API (opcional, gera√ß√£o de narrativas)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Qdrant (Vector DB)                           ‚îÇ
‚îÇ  - Armazena embeddings                                          ‚îÇ
‚îÇ  - Busca por similaridade                                       ‚îÇ
‚îÇ  - Sem LLM                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Claude Code - Camada de LLM

### 2.1 Op√ß√µes de Backend

O Claude Code suporta **3 op√ß√µes** de backend para acessar modelos Claude:

#### Op√ß√£o 1: Anthropic API (Padr√£o) ‚úÖ MAIS COMUM

**Endpoint**: `https://api.anthropic.com/v1/messages`

**Configura√ß√£o**:
```bash
# Automaticamente detectado se ANTHROPIC_API_KEY existe
export ANTHROPIC_API_KEY="sk-ant-api03-..."
```

**Modelos dispon√≠veis**:
- `claude-sonnet-4-5-20250929` (principal)
- `claude-haiku-4-5-20251001` (r√°pido/barato)
- `claude-opus-4-20250514` (m√°xima qualidade, se dispon√≠vel)

**Caracter√≠sticas**:
- ‚úÖ Setup mais simples
- ‚úÖ Acesso direto √†s features mais novas
- ‚úÖ Melhor documenta√ß√£o
- ‚ùå Requer API key da Anthropic
- ‚ùå Dados trafegam pela internet p√∫blica

**Autentica√ß√£o**:
```bash
# Via CLI
claude login

# Via env var
export ANTHROPIC_API_KEY="sk-ant-..."
```

#### Op√ß√£o 2: AWS Bedrock üè¢ ENTERPRISE

**Endpoint**: `https://bedrock-runtime.{region}.amazonaws.com/model/{model-id}/invoke`

**Configura√ß√£o**:
```bash
# Habilitar Bedrock
export CLAUDE_CODE_USE_BEDROCK=1
export AWS_REGION=us-east-1  # Obrigat√≥rio

# Autentica√ß√£o AWS (qualquer m√©todo)
aws configure  # Op√ß√£o 1: AWS CLI
# OU
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
# OU
export AWS_PROFILE="my-sso-profile"
aws sso login
```

**Modelos dispon√≠veis**:
- `global.anthropic.claude-sonnet-4-5-20250929-v1:0` (principal)
- `us.anthropic.claude-haiku-4-5-20251001-v1:0` (r√°pido)

**Caracter√≠sticas**:
- ‚úÖ Integrado ao ecossistema AWS
- ‚úÖ Pol√≠ticas IAM para controle de acesso
- ‚úÖ VPC endpoints (tr√°fego privado)
- ‚úÖ CloudWatch logging/monitoring
- ‚úÖ Or√ßamento/billing consolidado AWS
- ‚ùå Setup mais complexo
- ‚ùå Requer conta AWS e permiss√µes Bedrock
- ‚ùå Pode ter lat√™ncia adicional

**Diferen√ßas importantes**:
- `/login` e `/logout` desabilitados (usa AWS auth)
- Refresh autom√°tico de credenciais via SDK
- Suporte a SSO profiles

#### Op√ß√£o 3: Google Vertex AI üåê ENTERPRISE

**Endpoint**: `https://{region}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/anthropic/models/{model}:streamRawPredict`

**Configura√ß√£o**:
```bash
# Configurar Google Cloud
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
export GOOGLE_CLOUD_PROJECT="my-project-id"
export GOOGLE_CLOUD_REGION="us-central1"
```

**Modelos dispon√≠veis**:
- `claude-3-5-sonnet@20250929`
- `claude-3-5-haiku@20251001`

**Caracter√≠sticas**:
- ‚úÖ Integrado ao Google Cloud
- ‚úÖ Identity & Access Management (IAM)
- ‚úÖ Cloud Logging/Monitoring
- ‚úÖ Vertex AI Workbench integration
- ‚ùå Setup mais complexo
- ‚ùå Requer conta Google Cloud
- ‚ùå Geograficamente limitado a regi√µes do GCP

### 2.2 Qual Backend o Claude Code Usa Por Padr√£o?

**Detec√ß√£o Autom√°tica**:

```python
# Pseudo-c√≥digo do Claude Code
if os.getenv("CLAUDE_CODE_USE_BEDROCK") == "1":
    backend = "aws_bedrock"
elif os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    backend = "vertex_ai"
elif os.getenv("ANTHROPIC_API_KEY"):
    backend = "anthropic_api"
else:
    raise "No LLM backend configured!"
```

**Ordem de prioridade**:
1. Bedrock (se `CLAUDE_CODE_USE_BEDROCK=1`)
2. Vertex AI (se `GOOGLE_APPLICATION_CREDENTIALS` existe)
3. Anthropic API (padr√£o)

### 2.3 Como o LLM Interage com MCP Tools?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User: "Search my past conversations about Docker"              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Code                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  1. Envia prompt + contexto para LLM                           ‚îÇ
‚îÇ     POST /v1/messages                                          ‚îÇ
‚îÇ     {                                                          ‚îÇ
‚îÇ       "model": "claude-sonnet-4-5-20250929",                  ‚îÇ
‚îÇ       "messages": [{                                           ‚îÇ
‚îÇ         "role": "user",                                        ‚îÇ
‚îÇ         "content": "Search my past conversations about Docker" ‚îÇ
‚îÇ       }],                                                      ‚îÇ
‚îÇ       "tools": [                                               ‚îÇ
‚îÇ         {                                                      ‚îÇ
‚îÇ           "name": "mcp__claude-self-reflect__reflect_on_past", ‚îÇ
‚îÇ           "description": "Search past conversations...",       ‚îÇ
‚îÇ           "input_schema": {...}                               ‚îÇ
‚îÇ         },                                                     ‚îÇ
‚îÇ         ... mais 19 tools ...                                  ‚îÇ
‚îÇ       ]                                                        ‚îÇ
‚îÇ     }                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Anthropic API / Bedrock / Vertex AI                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. LLM processa e decide usar tool                            ‚îÇ
‚îÇ     Response:                                                  ‚îÇ
‚îÇ     {                                                          ‚îÇ
‚îÇ       "stop_reason": "tool_use",                              ‚îÇ
‚îÇ       "content": [{                                            ‚îÇ
‚îÇ         "type": "tool_use",                                    ‚îÇ
‚îÇ         "name": "mcp__claude-self-reflect__reflect_on_past",  ‚îÇ
‚îÇ         "input": {                                             ‚îÇ
‚îÇ           "query": "Docker",                                   ‚îÇ
‚îÇ           "mode": "full"                                       ‚îÇ
‚îÇ         }                                                      ‚îÇ
‚îÇ       }]                                                       ‚îÇ
‚îÇ     }                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Code                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. Extrai tool_use e chama MCP server                        ‚îÇ
‚îÇ     stdio ‚Üí run-mcp.sh ‚Üí python server.py                     ‚îÇ
‚îÇ     Tool: reflect_on_past(query="Docker", mode="full")       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Claude Self-Reflect MCP Server                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  4. Executa busca (SEM usar LLM)                               ‚îÇ
‚îÇ     a. Gera embedding do query "Docker"                        ‚îÇ
‚îÇ        - Local: FastEmbed                                      ‚îÇ
‚îÇ        - Cloud: Voyage AI                                      ‚îÇ
‚îÇ     b. Busca no Qdrant por similaridade                        ‚îÇ
‚îÇ     c. Aplica memory decay                                     ‚îÇ
‚îÇ     d. Formata resultados                                      ‚îÇ
‚îÇ     e. Retorna JSON                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Code                                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  5. Envia resultado de volta ao LLM                            ‚îÇ
‚îÇ     POST /v1/messages                                          ‚îÇ
‚îÇ     {                                                          ‚îÇ
‚îÇ       "messages": [                                            ‚îÇ
‚îÇ         ... original message ...,                              ‚îÇ
‚îÇ         {                                                      ‚îÇ
‚îÇ           "role": "assistant",                                 ‚îÇ
‚îÇ           "content": [{ "type": "tool_use", ... }]            ‚îÇ
‚îÇ         },                                                     ‚îÇ
‚îÇ         {                                                      ‚îÇ
‚îÇ           "role": "user",                                      ‚îÇ
‚îÇ           "content": [{                                        ‚îÇ
‚îÇ             "type": "tool_result",                            ‚îÇ
‚îÇ             "tool_use_id": "...",                             ‚îÇ
‚îÇ             "content": "[{...search results...}]"            ‚îÇ
‚îÇ           }]                                                   ‚îÇ
‚îÇ         }                                                      ‚îÇ
‚îÇ       ]                                                        ‚îÇ
‚îÇ     }                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Anthropic API / Bedrock / Vertex AI                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  6. LLM sintetiza resposta final                               ‚îÇ
‚îÇ     Response:                                                  ‚îÇ
‚îÇ     {                                                          ‚îÇ
‚îÇ       "content": [{                                            ‚îÇ
‚îÇ         "type": "text",                                        ‚îÇ
‚îÇ         "text": "Found 3 conversations about Docker:\n        ‚îÇ
‚îÇ                  1. Docker compose issues (2 weeks ago)...\n  ‚îÇ
‚îÇ                  2. Container memory limits (1 month ago)..." ‚îÇ
‚îÇ       }]                                                       ‚îÇ
‚îÇ     }                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Claude Code                                  ‚îÇ
‚îÇ  7. Renderiza resposta para o usu√°rio                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pontos importantes**:
- ‚úÖ O LLM decide **quando** invocar tools
- ‚úÖ O LLM decide **quais** tools invocar
- ‚úÖ O LLM decide **como** interpretar resultados
- ‚ùå O MCP server **N√ÉO** tem acesso ao LLM
- ‚ùå O MCP server **N√ÉO** decide nada "inteligente"

---

## 3. Claude Self-Reflect - Sem LLM Pr√≥prio

### 3.1 O Que o MCP Server Faz (SEM LLM)

```python
# mcp-server/src/server.py

@mcp.tool()
async def reflect_on_past(query: str, mode: str = "full"):
    """Search past conversations - NO LLM NEEDED."""

    # 1. Gera embedding do query (matem√°tica pura)
    if use_local:
        embedding = fastembed.embed(query)  # Modelo local
    else:
        embedding = voyage.embed(query)     # API Voyage

    # 2. Busca vetorial (algoritmo matem√°tico)
    results = await qdrant.search(
        collection_name=collection,
        query_vector=embedding,
        limit=10
    )

    # 3. Aplica memory decay (f√≥rmula matem√°tica)
    for result in results:
        age_days = (now - result.timestamp).days
        decay_factor = exp(-age_days / 90)  # 90-day half-life
        result.score *= decay_factor

    # 4. Formata e retorna (string processing)
    return format_results(results)
```

**Nenhum LLM envolvido!** Apenas:
- ‚úÖ Modelos de embedding (transformers matem√°ticos)
- ‚úÖ Busca vetorial (cosine similarity)
- ‚úÖ Processamento de strings
- ‚úÖ Formata√ß√£o de dados

### 3.2 Embeddings ‚â† LLM

**Diferen√ßa crucial**:

| Aspecto | Embedding Model | LLM (Claude) |
|---------|----------------|--------------|
| **Prop√≥sito** | Converter texto ‚Üí vetor num√©rico | Gerar texto inteligente |
| **Tamanho** | ~100MB (FastEmbed) | ~100GB+ (Claude) |
| **Output** | Array de floats [0.1, -0.5, ...] | Texto natural |
| **Racioc√≠nio** | ‚ùå N√£o raciocina | ‚úÖ Raciocina |
| **Custo** | Gr√°tis (local) ou $0.0001/query | $0.003-$0.015/1K tokens |
| **Lat√™ncia** | 10-50ms | 500-5000ms |

**Embedding models usados**:

```python
# Local (padr√£o) - 384 dimens√µes
model = "sentence-transformers/all-MiniLM-L6-v2"
# Gratuito, roda local, 100MB, 20ms

# Cloud (opcional) - 1024 dimens√µes
model = "voyage-3"
# Pago ($0.00012/1K tokens), API, melhor qualidade, 50ms
```

### 3.3 EXCE√á√ÉO: v7.0 Batch Narratives

**A √öNICA parte que usa LLM da Anthropic**:

```python
# docs/design/batch_import_all_projects.py

def batch_generate_narratives(conversations: list):
    """Usa Anthropic Batch API para gerar narrativas."""

    client = anthropic.Anthropic(
        api_key=os.getenv("ANTHROPIC_API_KEY")
    )

    # Cria batch job
    batch = client.batches.create(
        requests=[
            {
                "custom_id": f"conv-{i}",
                "params": {
                    "model": "claude-sonnet-4-5-20250929",
                    "messages": [{
                        "role": "user",
                        "content": f"Transform this conversation into a structured narrative:\n{conv}"
                    }]
                }
            }
            for i, conv in enumerate(conversations)
        ]
    )

    return batch.id
```

**Por que isso √© diferente**:
- ‚ùå **N√ÉO** √© o MCP server que chama
- ‚úÖ √â um **script separado** (`batch_import_all_projects.py`)
- ‚úÖ Roda **offline** (n√£o em tempo real)
- ‚úÖ Usa **Batch API** (50% mais barato)
- ‚úÖ **Opcional** (requer ANTHROPIC_API_KEY)

**Prop√≥sito**:
- Gerar narrativas ricas de conversas antigas
- Melhorar qualidade de busca (0.074 ‚Üí 0.691 score)
- Extrair metadata (tools, files, concepts)

---

## 4. Compara√ß√£o: API Direta vs Claude Code

### 4.1 Chamando Anthropic API Diretamente

```python
import anthropic

client = anthropic.Anthropic(api_key="sk-ant-...")

# Chamada manual
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": "Search my past conversations about Docker"
    }]
)

print(response.content[0].text)
# Output: "I don't have access to your past conversations.
#          I'm starting fresh with no memory of previous chats."
```

**Problema**: Claude n√£o tem mem√≥ria!

### 4.2 Via Claude Code + MCP

```bash
# Claude Code CLI
claude

> Search my past conversations about Docker

# Claude Code automaticamente:
# 1. Detecta inten√ß√£o de buscar mem√≥ria
# 2. Invoca mcp__claude-self-reflect__reflect_on_past("Docker")
# 3. MCP server busca no Qdrant
# 4. LLM sintetiza resultados
# Output: "Found 3 conversations about Docker:
#          1. Docker compose issues (2 weeks ago)...
#          2. Container memory (1 month ago)..."
```

**Benef√≠cio**: Claude TEM mem√≥ria via MCP!

### 4.3 Diferen√ßas Chave

| Aspecto | API Direta | Claude Code + MCP |
|---------|-----------|-------------------|
| **Mem√≥ria** | ‚ùå Nenhuma | ‚úÖ Ilimitada (Qdrant) |
| **Tools** | ‚ö†Ô∏è Manual (via tool calling) | ‚úÖ Autom√°tico (MCP) |
| **Contexto** | Apenas conversa atual | Hist√≥rico completo |
| **Setup** | API key | CLI + MCP server |
| **Custo** | Apenas API | API + infraestrutura |
| **Lat√™ncia** | ~500ms | ~700ms (+ MCP call) |

---

## 5. Fluxo Completo de Dados

### 5.1 Write Path (Indexa√ß√£o)

```
Conversa no Claude Code
        ‚îÇ
        ‚ñº
~/.claude/projects/{project}/{timestamp}.jsonl
        ‚îÇ
        ‚ñº
Watcher detecta novo arquivo
        ‚îÇ
        ‚ñº
streaming-watcher.py (Python)
‚îú‚îÄ Parse JSONL
‚îú‚îÄ Chunk messages
‚îú‚îÄ Generate embeddings (FastEmbed ou Voyage)
‚îî‚îÄ Upload para Qdrant
        ‚îÇ
        ‚ñº
Qdrant Vector Database
‚îî‚îÄ Armazena {embedding, metadata, payload}
```

**Nenhum LLM envolvido** (exceto v7.0 narratives opcionais)

### 5.2 Read Path (Busca)

```
User: "Search Docker issues"
        ‚îÇ
        ‚ñº
Claude Code CLI
‚îú‚îÄ Parse comando
‚îî‚îÄ Send to LLM (Anthropic/Bedrock/Vertex)
        ‚îÇ
        ‚ñº
LLM decide usar tool
‚îî‚îÄ tool_use: reflect_on_past("Docker")
        ‚îÇ
        ‚ñº
Claude Code invoca MCP server
‚îî‚îÄ stdio ‚Üí run-mcp.sh ‚Üí python server.py
        ‚îÇ
        ‚ñº
MCP Server (NO LLM)
‚îú‚îÄ Generate embedding("Docker")
‚îú‚îÄ Search Qdrant
‚îú‚îÄ Apply memory decay
‚îî‚îÄ Format results
        ‚îÇ
        ‚ñº
Return results to Claude Code
        ‚îÇ
        ‚ñº
Claude Code ‚Üí LLM
‚îî‚îÄ Synthesize response with results
        ‚îÇ
        ‚ñº
Render to user
```

**LLM usado apenas 2x**:
1. Decidir invocar tool
2. Sintetizar resposta final

---

## 6. Custos e Performance

### 6.1 Custos por Componente

#### Claude Code (LLM)

| Backend | Modelo | Input | Output | Batch (50% off) |
|---------|--------|-------|--------|----------------|
| **Anthropic API** | Sonnet 4.5 | $3/MTok | $15/MTok | $1.50/MTok |
| **Anthropic API** | Haiku 4.5 | $0.80/MTok | $4/MTok | $0.40/MTok |
| **AWS Bedrock** | Sonnet 4.5 | $3/MTok | $15/MTok | ‚ùå N/A |
| **AWS Bedrock** | Haiku 4.5 | $0.80/MTok | $4/MTok | ‚ùå N/A |
| **Vertex AI** | Sonnet 3.5 | $3/MTok | $15/MTok | ‚ùå N/A |

**Custo m√©dio de query com MCP**:
```
User query: "Search Docker" (~10 tokens)
LLM decision: (~100 tokens)
Tool result: (~500 tokens)
LLM synthesis: (~200 tokens)

Total: 10 input + 200 output = ~$0.003
```

#### Claude Self-Reflect (MCP Server)

| Componente | Custo |
|-----------|-------|
| **Local embeddings** | $0 (FastEmbed) |
| **Cloud embeddings** | $0.00012/1K tokens (Voyage) |
| **Qdrant** | $0 (self-hosted Docker) |
| **Python processing** | $0 (compute neglig√≠vel) |

**Custo m√©dio de busca**:
```
Query: "Search Docker" (~2 tokens)
Embedding: $0.00000024 (Voyage) ou $0 (local)
Qdrant: $0
Total: ~$0
```

#### v7.0 Narratives (Opcional)

| Opera√ß√£o | Custo |
|----------|-------|
| **Narrative generation** | $0.012/conversation (Batch API) |
| **Standard API** | $0.025/conversation |
| **Savings** | 50% |

**ROI**: Melhoria de 9.3x na qualidade de busca por $0.012/conv

### 6.2 Performance

| Opera√ß√£o | Lat√™ncia | Onde |
|----------|----------|------|
| **LLM call** | 500-5000ms | Anthropic/Bedrock/Vertex |
| **Embedding** | 10-50ms | Local ou Voyage |
| **Qdrant search** | 3-15ms | Docker local |
| **Total MCP call** | 20-100ms | Soma embedding + search |
| **Full query** | 700-5200ms | LLM + MCP |

**Gargalo**: LLM (10-100x mais lento que MCP)

---

## 7. Configura√ß√£o Recomendada

### 7.1 Para Desenvolvedores Individuais

```bash
# LLM: Anthropic API (simples)
export ANTHROPIC_API_KEY="sk-ant-..."

# Embeddings: Local (gr√°tis, privado)
export PREFER_LOCAL_EMBEDDINGS=true

# MCP: stdio (baixa lat√™ncia)
claude mcp add claude-self-reflect \
  /path/to/run-mcp.sh

# Narratives: Desabilitado (economizar)
# (sem ANTHROPIC_API_KEY em .env do projeto)
```

**Custo mensal estimado**: $5-20 (apenas LLM queries)

### 7.2 Para Times (Enterprise)

```bash
# LLM: AWS Bedrock (controle, billing)
export CLAUDE_CODE_USE_BEDROCK=1
export AWS_REGION=us-east-1
aws sso login --profile work

# Embeddings: Voyage (melhor qualidade)
export PREFER_LOCAL_EMBEDDINGS=false
export VOYAGE_KEY="pa-..."

# MCP: stdio (por enquanto, HTTP futuro)
claude mcp add claude-self-reflect \
  /path/to/run-mcp.sh

# Narratives: Habilitado (ROI positivo)
export ANTHROPIC_API_KEY="sk-ant-..."
docker compose --profile batch-automation up -d
```

**Custo mensal estimado**:
- LLM (Bedrock): $50-500/user
- Embeddings (Voyage): $1-10/user
- Narratives: $10-50/user
- **Total**: $61-560/user/m√™s

### 7.3 Para Research/Academia

```bash
# LLM: Anthropic API (melhor modelo)
export ANTHROPIC_API_KEY="sk-ant-..."

# Embeddings: Local (sem custo, reproduz√≠vel)
export PREFER_LOCAL_EMBEDDINGS=true

# MCP: stdio
claude mcp add claude-self-reflect \
  /path/to/run-mcp.sh

# Narratives: Habilitado para experimentos
export ANTHROPIC_API_KEY="sk-ant-..."
```

**Custo mensal estimado**: $10-50 (uso moderado)

---

## 8. Perguntas Frequentes

### Q1: O MCP server precisa de API key da Anthropic?

**R**: **N√ÉO**, o MCP server N√ÉO usa LLM da Anthropic para buscas normais.

**Exce√ß√£o**: v7.0 batch narratives (opcional) usa Batch API para gerar narrativas.

### Q2: Qual modelo de embedding o Claude Code usa?

**R**: O **Claude Code** n√£o usa embeddings. O **Claude Self-Reflect MCP Server** usa:
- Local: `sentence-transformers/all-MiniLM-L6-v2` (384d, gr√°tis)
- Cloud: `voyage-3` (1024d, $0.00012/1K tokens)

### Q3: Posso usar Claude Self-Reflect sem internet?

**R**: **SIM**, se usar:
- Local embeddings (`PREFER_LOCAL_EMBEDDINGS=true`)
- Qdrant local (Docker)

**Mas ainda precisa de internet para**:
- Claude Code chamar LLM (Anthropic/Bedrock/Vertex)

### Q4: O MCP server roda o modelo Claude localmente?

**R**: **N√ÉO**. O MCP server:
- ‚ùå N√ÉO tem modelo Claude
- ‚ùå N√ÉO faz infer√™ncia de LLM
- ‚úÖ Apenas busca vetorial + formata√ß√£o

O modelo Claude roda em:
- ‚òÅÔ∏è Anthropic API (cloud)
- ‚òÅÔ∏è AWS Bedrock (cloud)
- ‚òÅÔ∏è Google Vertex AI (cloud)

**N√£o h√° op√ß√£o de rodar Claude localmente** (modelo √© propriet√°rio)

### Q5: Qual a diferen√ßa entre FastEmbed e Voyage?

| Aspecto | FastEmbed | Voyage |
|---------|-----------|--------|
| **Onde roda** | Local | Cloud API |
| **Custo** | Gr√°tis | $0.00012/1K tokens |
| **Dimens√µes** | 384 | 1024 |
| **Qualidade** | Boa | Melhor |
| **Lat√™ncia** | 10-20ms | 40-60ms |
| **Privacidade** | 100% local | Envia texto para API |
| **Setup** | Zero config | API key |

**Recomenda√ß√£o**:
- **FastEmbed**: Default (gr√°tis, privado, bom suficiente)
- **Voyage**: Se precisa de m√°xima qualidade e pode pagar

### Q6: Bedrock √© mais barato que API direta?

**R**: **Mesmo pre√ßo** para modelos Claude:
- API direta: $3/MTok input, $15/MTok output
- Bedrock: $3/MTok input, $15/MTok output

**Mas Bedrock oferece**:
- ‚úÖ Billing consolidado AWS
- ‚úÖ VPC privado (sem tr√°fego p√∫blico)
- ‚úÖ IAM policies granulares
- ‚úÖ CloudWatch integration

**Trade-off**: Complexidade de setup vs benef√≠cios enterprise

### Q7: v7.0 narratives compensa o custo?

**R**: **Depende do uso**:

**Custo**:
- $0.012/conversation via Batch API
- 1000 conversas = $12

**Benef√≠cio**:
- Search quality: 0.074 ‚Üí 0.691 (9.3x melhor)
- Token compression: 82% redu√ß√£o
- Metadata: tools, files, concepts

**ROI positivo se**:
- Voc√™ faz muitas buscas
- Qualidade de busca √© cr√≠tica
- Tem or√ßamento para melhorias

**Pular se**:
- Or√ßamento apertado
- Poucas conversas (<100)
- Busca b√°sica funciona bem

---

## 9. Conclus√£o

### Resumo da Arquitetura de LLM

**Claude Code**:
- ‚úÖ **TEM** LLM (Claude via API/Bedrock/Vertex)
- ‚úÖ **DECIDE** quando usar MCP tools
- ‚úÖ **SINTETIZA** respostas finais
- üí∞ **CUSTO**: $0.003-0.015/query

**Claude Self-Reflect MCP Server**:
- ‚ùå **N√ÉO TEM** LLM pr√≥prio
- ‚úÖ **BUSCA** vetorial via embeddings
- ‚úÖ **FORMATA** resultados
- üí∞ **CUSTO**: ~$0/query (local) ou ~$0.0001/query (Voyage)

**Exce√ß√£o v7.0**:
- ‚ö†Ô∏è **USA** Anthropic Batch API
- ‚úÖ **GERA** narrativas offline
- ‚úÖ **OPCIONAL** (requer API key)
- üí∞ **CUSTO**: $0.012/conversation

### Diagrama Mental Simples

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Voc√™ faz pergunta  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude Code                 ‚îÇ
‚îÇ  (Tem LLM - Pago)           ‚îÇ
‚îÇ  ‚îú‚îÄ Raciocina               ‚îÇ
‚îÇ  ‚îú‚îÄ Decide invocar tools    ‚îÇ
‚îÇ  ‚îî‚îÄ Sintetiza resposta      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude Self-Reflect         ‚îÇ
‚îÇ  (Sem LLM - Gr√°tis)         ‚îÇ
‚îÇ  ‚îú‚îÄ Busca vetorial          ‚îÇ
‚îÇ  ‚îú‚îÄ Formata dados           ‚îÇ
‚îÇ  ‚îî‚îÄ Retorna resultados      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Recomenda√ß√£o Final

**Para maioria dos usu√°rios**:
```bash
LLM: Anthropic API (simples)
Embeddings: Local (gr√°tis)
Narratives: Desabilitado (economizar)
```

**Para empresas**:
```bash
LLM: AWS Bedrock (controle)
Embeddings: Voyage (qualidade)
Narratives: Habilitado (ROI)
```

**Princ√≠pio chave**:
- LLM = Intelig√™ncia (caro, Claude Code)
- MCP = Mem√≥ria (barato, Self-Reflect)
- Separa√ß√£o = Efici√™ncia

---

**Documento atualizado**: 2025-01-05
**Vers√£o**: 1.0
